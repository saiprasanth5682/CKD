{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d57c925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dc660b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bp</th>\n",
       "      <th>sg</th>\n",
       "      <th>al</th>\n",
       "      <th>su</th>\n",
       "      <th>rbc</th>\n",
       "      <th>pc</th>\n",
       "      <th>pcc</th>\n",
       "      <th>ba</th>\n",
       "      <th>bgr</th>\n",
       "      <th>...</th>\n",
       "      <th>pcv</th>\n",
       "      <th>wc</th>\n",
       "      <th>rc</th>\n",
       "      <th>htn</th>\n",
       "      <th>dm</th>\n",
       "      <th>cad</th>\n",
       "      <th>appet</th>\n",
       "      <th>pe</th>\n",
       "      <th>ane</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normal</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>121.0</td>\n",
       "      <td>...</td>\n",
       "      <td>44</td>\n",
       "      <td>7800</td>\n",
       "      <td>5.2</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>good</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>ckd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.020</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normal</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>6000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>good</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>ckd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.010</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>423.0</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>poor</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>ckd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.005</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>abnormal</td>\n",
       "      <td>present</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>117.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>6700</td>\n",
       "      <td>3.9</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>poor</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>ckd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.010</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>106.0</td>\n",
       "      <td>...</td>\n",
       "      <td>35</td>\n",
       "      <td>7300</td>\n",
       "      <td>4.6</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>good</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>ckd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>55.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>140.0</td>\n",
       "      <td>...</td>\n",
       "      <td>47</td>\n",
       "      <td>6700</td>\n",
       "      <td>4.9</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>good</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>notckd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>42.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>75.0</td>\n",
       "      <td>...</td>\n",
       "      <td>54</td>\n",
       "      <td>7800</td>\n",
       "      <td>6.2</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>good</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>notckd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>12.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>49</td>\n",
       "      <td>6600</td>\n",
       "      <td>5.4</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>good</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>notckd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>114.0</td>\n",
       "      <td>...</td>\n",
       "      <td>51</td>\n",
       "      <td>7200</td>\n",
       "      <td>5.9</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>good</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>notckd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>58.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>131.0</td>\n",
       "      <td>...</td>\n",
       "      <td>53</td>\n",
       "      <td>6800</td>\n",
       "      <td>6.1</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>good</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>notckd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age    bp     sg   al   su     rbc        pc         pcc          ba  \\\n",
       "id                                                                           \n",
       "0    48.0  80.0  1.020  1.0  0.0     NaN    normal  notpresent  notpresent   \n",
       "1     7.0  50.0  1.020  4.0  0.0     NaN    normal  notpresent  notpresent   \n",
       "2    62.0  80.0  1.010  2.0  3.0  normal    normal  notpresent  notpresent   \n",
       "3    48.0  70.0  1.005  4.0  0.0  normal  abnormal     present  notpresent   \n",
       "4    51.0  80.0  1.010  2.0  0.0  normal    normal  notpresent  notpresent   \n",
       "..    ...   ...    ...  ...  ...     ...       ...         ...         ...   \n",
       "395  55.0  80.0  1.020  0.0  0.0  normal    normal  notpresent  notpresent   \n",
       "396  42.0  70.0  1.025  0.0  0.0  normal    normal  notpresent  notpresent   \n",
       "397  12.0  80.0  1.020  0.0  0.0  normal    normal  notpresent  notpresent   \n",
       "398  17.0  60.0  1.025  0.0  0.0  normal    normal  notpresent  notpresent   \n",
       "399  58.0  80.0  1.025  0.0  0.0  normal    normal  notpresent  notpresent   \n",
       "\n",
       "       bgr  ...  pcv    wc   rc  htn   dm cad appet   pe  ane classification  \n",
       "id          ...                                                               \n",
       "0    121.0  ...   44  7800  5.2  yes  yes  no  good   no   no            ckd  \n",
       "1      NaN  ...   38  6000  NaN   no   no  no  good   no   no            ckd  \n",
       "2    423.0  ...   31  7500  NaN   no  yes  no  poor   no  yes            ckd  \n",
       "3    117.0  ...   32  6700  3.9  yes   no  no  poor  yes  yes            ckd  \n",
       "4    106.0  ...   35  7300  4.6   no   no  no  good   no   no            ckd  \n",
       "..     ...  ...  ...   ...  ...  ...  ...  ..   ...  ...  ...            ...  \n",
       "395  140.0  ...   47  6700  4.9   no   no  no  good   no   no         notckd  \n",
       "396   75.0  ...   54  7800  6.2   no   no  no  good   no   no         notckd  \n",
       "397  100.0  ...   49  6600  5.4   no   no  no  good   no   no         notckd  \n",
       "398  114.0  ...   51  7200  5.9   no   no  no  good   no   no         notckd  \n",
       "399  131.0  ...   53  6800  6.1   no   no  no  good   no   no         notckd  \n",
       "\n",
       "[400 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"kidney.csv\",index_col=['id'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a8e2b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "0         ckd\n",
       "1         ckd\n",
       "2         ckd\n",
       "3         ckd\n",
       "4         ckd\n",
       "        ...  \n",
       "395    notckd\n",
       "396    notckd\n",
       "397    notckd\n",
       "398    notckd\n",
       "399    notckd\n",
       "Name: classification, Length: 400, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=data.iloc[:,:-1]\n",
    "x\n",
    "y=data['classification']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95cbe545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Counts:\n",
      "ckd       248\n",
      "notckd    150\n",
      "ckd\\t       2\n",
      "Name: classification, dtype: int64\n",
      "Number of Classes: 3\n"
     ]
    }
   ],
   "source": [
    "class_counts = y.value_counts()\n",
    "num_classes = len(class_counts)\n",
    "print(\"Class Counts:\")\n",
    "print(class_counts)\n",
    "print(\"Number of Classes:\", num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb9f7d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Counts:\n",
      "ckd       250\n",
      "notckd    150\n",
      "Name: classification, dtype: int64\n",
      "Number of Classes: 2\n"
     ]
    }
   ],
   "source": [
    "# Replace \"ckd\\t\" with \"ckd\" in the 'classification' column\n",
    "y = y.replace(\"ckd\\t\", \"ckd\")\n",
    "\n",
    "# Check the updated class counts and number of classes\n",
    "class_counts = y.value_counts()\n",
    "num_classes = len(class_counts)\n",
    "print(\"Class Counts:\")\n",
    "print(class_counts)\n",
    "print(\"Number of Classes:\", num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "976937e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bp</th>\n",
       "      <th>sg</th>\n",
       "      <th>al</th>\n",
       "      <th>su</th>\n",
       "      <th>bgr</th>\n",
       "      <th>bu</th>\n",
       "      <th>sc</th>\n",
       "      <th>sod</th>\n",
       "      <th>pot</th>\n",
       "      <th>...</th>\n",
       "      <th>htn_yes</th>\n",
       "      <th>dm_\\tyes</th>\n",
       "      <th>dm_ yes</th>\n",
       "      <th>dm_no</th>\n",
       "      <th>dm_yes</th>\n",
       "      <th>cad_no</th>\n",
       "      <th>cad_yes</th>\n",
       "      <th>appet_poor</th>\n",
       "      <th>pe_yes</th>\n",
       "      <th>ane_yes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.020</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.010</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>423.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.005</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>111.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.010</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>55.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>150.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>42.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>141.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>12.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>137.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>58.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>141.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age    bp     sg   al   su    bgr    bu   sc    sod  pot  ...  htn_yes  \\\n",
       "id                                                              ...            \n",
       "0    48.0  80.0  1.020  1.0  0.0  121.0  36.0  1.2    NaN  NaN  ...        1   \n",
       "1     7.0  50.0  1.020  4.0  0.0    NaN  18.0  0.8    NaN  NaN  ...        0   \n",
       "2    62.0  80.0  1.010  2.0  3.0  423.0  53.0  1.8    NaN  NaN  ...        0   \n",
       "3    48.0  70.0  1.005  4.0  0.0  117.0  56.0  3.8  111.0  2.5  ...        1   \n",
       "4    51.0  80.0  1.010  2.0  0.0  106.0  26.0  1.4    NaN  NaN  ...        0   \n",
       "..    ...   ...    ...  ...  ...    ...   ...  ...    ...  ...  ...      ...   \n",
       "395  55.0  80.0  1.020  0.0  0.0  140.0  49.0  0.5  150.0  4.9  ...        0   \n",
       "396  42.0  70.0  1.025  0.0  0.0   75.0  31.0  1.2  141.0  3.5  ...        0   \n",
       "397  12.0  80.0  1.020  0.0  0.0  100.0  26.0  0.6  137.0  4.4  ...        0   \n",
       "398  17.0  60.0  1.025  0.0  0.0  114.0  50.0  1.0  135.0  4.9  ...        0   \n",
       "399  58.0  80.0  1.025  0.0  0.0  131.0  18.0  1.1  141.0  3.5  ...        0   \n",
       "\n",
       "     dm_\\tyes  dm_ yes  dm_no  dm_yes  cad_no  cad_yes  appet_poor  pe_yes  \\\n",
       "id                                                                           \n",
       "0           0        0      0       1       1        0           0       0   \n",
       "1           0        0      1       0       1        0           0       0   \n",
       "2           0        0      0       1       1        0           1       0   \n",
       "3           0        0      1       0       1        0           1       1   \n",
       "4           0        0      1       0       1        0           0       0   \n",
       "..        ...      ...    ...     ...     ...      ...         ...     ...   \n",
       "395         0        0      1       0       1        0           0       0   \n",
       "396         0        0      1       0       1        0           0       0   \n",
       "397         0        0      1       0       1        0           0       0   \n",
       "398         0        0      1       0       1        0           0       0   \n",
       "399         0        0      1       0       1        0           0       0   \n",
       "\n",
       "     ane_yes  \n",
       "id            \n",
       "0          0  \n",
       "1          0  \n",
       "2          1  \n",
       "3          1  \n",
       "4          0  \n",
       "..       ...  \n",
       "395        0  \n",
       "396        0  \n",
       "397        0  \n",
       "398        0  \n",
       "399        0  \n",
       "\n",
       "[400 rows x 204 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encoding\n",
    "x=pd.get_dummies(x,drop_first=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93da1dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bp</th>\n",
       "      <th>sg</th>\n",
       "      <th>al</th>\n",
       "      <th>su</th>\n",
       "      <th>bgr</th>\n",
       "      <th>bu</th>\n",
       "      <th>sc</th>\n",
       "      <th>sod</th>\n",
       "      <th>pot</th>\n",
       "      <th>...</th>\n",
       "      <th>htn_yes</th>\n",
       "      <th>dm_\\tyes</th>\n",
       "      <th>dm_ yes</th>\n",
       "      <th>dm_no</th>\n",
       "      <th>dm_yes</th>\n",
       "      <th>cad_no</th>\n",
       "      <th>cad_yes</th>\n",
       "      <th>appet_poor</th>\n",
       "      <th>pe_yes</th>\n",
       "      <th>ane_yes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.020</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.010</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>423.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.005</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>111.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.010</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>55.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>150.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>42.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>141.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>12.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>137.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>58.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>141.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age    bp     sg   al   su    bgr    bu   sc    sod  pot  ...  htn_yes  \\\n",
       "id                                                              ...            \n",
       "0    48.0  80.0  1.020  1.0  0.0  121.0  36.0  1.2    0.0  0.0  ...        1   \n",
       "1     7.0  50.0  1.020  4.0  0.0    0.0  18.0  0.8    0.0  0.0  ...        0   \n",
       "2    62.0  80.0  1.010  2.0  3.0  423.0  53.0  1.8    0.0  0.0  ...        0   \n",
       "3    48.0  70.0  1.005  4.0  0.0  117.0  56.0  3.8  111.0  2.5  ...        1   \n",
       "4    51.0  80.0  1.010  2.0  0.0  106.0  26.0  1.4    0.0  0.0  ...        0   \n",
       "..    ...   ...    ...  ...  ...    ...   ...  ...    ...  ...  ...      ...   \n",
       "395  55.0  80.0  1.020  0.0  0.0  140.0  49.0  0.5  150.0  4.9  ...        0   \n",
       "396  42.0  70.0  1.025  0.0  0.0   75.0  31.0  1.2  141.0  3.5  ...        0   \n",
       "397  12.0  80.0  1.020  0.0  0.0  100.0  26.0  0.6  137.0  4.4  ...        0   \n",
       "398  17.0  60.0  1.025  0.0  0.0  114.0  50.0  1.0  135.0  4.9  ...        0   \n",
       "399  58.0  80.0  1.025  0.0  0.0  131.0  18.0  1.1  141.0  3.5  ...        0   \n",
       "\n",
       "     dm_\\tyes  dm_ yes  dm_no  dm_yes  cad_no  cad_yes  appet_poor  pe_yes  \\\n",
       "id                                                                           \n",
       "0           0        0      0       1       1        0           0       0   \n",
       "1           0        0      1       0       1        0           0       0   \n",
       "2           0        0      0       1       1        0           1       0   \n",
       "3           0        0      1       0       1        0           1       1   \n",
       "4           0        0      1       0       1        0           0       0   \n",
       "..        ...      ...    ...     ...     ...      ...         ...     ...   \n",
       "395         0        0      1       0       1        0           0       0   \n",
       "396         0        0      1       0       1        0           0       0   \n",
       "397         0        0      1       0       1        0           0       0   \n",
       "398         0        0      1       0       1        0           0       0   \n",
       "399         0        0      1       0       1        0           0       0   \n",
       "\n",
       "     ane_yes  \n",
       "id            \n",
       "0          0  \n",
       "1          0  \n",
       "2          1  \n",
       "3          1  \n",
       "4          0  \n",
       "..       ...  \n",
       "395        0  \n",
       "396        0  \n",
       "397        0  \n",
       "398        0  \n",
       "399        0  \n",
       "\n",
       "[400 rows x 204 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=pd.DataFrame.fillna(x,0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94990add",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder,StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7cf617c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Temp=LabelEncoder()\n",
    "y=Temp.fit_transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e64d8144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.82521253e+01, -1.06697836e+02,  1.80672929e+01, ...,\n",
       "        -2.42863814e-01,  1.90303838e-01,  6.79309491e-01],\n",
       "       [-1.43216552e+02, -1.02670068e+02,  7.43787860e+00, ...,\n",
       "         2.42816118e-02, -4.17997170e-01, -3.96473380e-02],\n",
       "       [ 2.83665905e+02, -1.26579329e+02,  9.07434209e+00, ...,\n",
       "         7.07372402e-01, -6.71072312e-01,  8.23677667e-02],\n",
       "       ...,\n",
       "       [-3.48463583e+01,  2.12466035e+01, -3.56440936e+01, ...,\n",
       "        -5.22102655e-02,  7.09491883e-02,  6.13327744e-02],\n",
       "       [-1.89467882e+01,  2.56959886e+01, -1.47478735e+01, ...,\n",
       "        -1.55454025e-01,  4.25409516e-02, -1.46600543e-01],\n",
       "       [-2.20233440e+00,  2.00884948e+01, -4.45123085e+01, ...,\n",
       "         9.73316882e-02, -5.73506969e-03, -2.01243593e-02]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "model=PCA(n_components=20)\n",
    "pca=model.fit_transform(x)\n",
    "pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89fd22d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrpca,xtepca,ytrpca,ytepca=train_test_split(pca,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "339a4b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db2f7ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architecture of the neural network\n",
    "m6 = keras.Sequential()\n",
    "m6.add(keras.layers.Dense(units=64, activation='relu', input_dim=xtr.shape[1]))\n",
    "m6.add(keras.layers.Dropout(0.5))\n",
    "m6.add(keras.layers.Dense(units=64, activation='relu'))\n",
    "m6.add(keras.layers.Dropout(0.2))\n",
    "m6.add(keras.layers.Dense(units=64, activation='relu'))\n",
    "m6.add(keras.layers.Dropout(0.2))\n",
    "m6.add(keras.layers.Dense(units=64, activation='relu'))\n",
    "m6.add(keras.layers.Dense(units=1, activation='softmax'))  # For binary classification, change activation as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8807db22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                256       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12801 (50.00 KB)\n",
      "Trainable params: 12801 (50.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m6.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "m6.summary()# Adjust loss and optimizer as needed\n",
    "\n",
    "# Train the model\n",
    "#m6.fit(xtr,ytr, epochs=10, batch_size=32)  # Adjust epochs and batch_size as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08daf2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saipr\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(32, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/7 [========================>.....] - ETA: 0s - loss: 0.0000e+00 - accuracy: 0.3802"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saipr\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 6s 143ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 0s 38ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 0s 36ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 0s 53ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 0s 39ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 31/50\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 32/50\n",
      "7/7 [==============================] - 0s 39ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 33/50\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 34/50\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 35/50\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 36/50\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 37/50\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 38/50\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 39/50\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 40/50\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 41/50\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 42/50\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 43/50\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 44/50\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 45/50\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 46/50\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 47/50\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 48/50\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 49/50\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n",
      "Epoch 50/50\n",
      "7/7 [==============================] - 0s 42ms/step - loss: 0.0000e+00 - accuracy: 0.3616 - val_loss: 0.0000e+00 - val_accuracy: 0.4464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x23d4895dd00>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m6.fit(xtr,ytr, epochs=50, batch_size=32,validation_split=0.2)  # Adjust epochs and batch_size as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "057e32cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialzing empty lists to append all model's name and corresponding name\n",
    "acc = []\n",
    "model = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a67a1b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "NB=GaussianNB()\n",
    "NB.fit(xtrpca,ytrpca)\n",
    "ypr1=NB.predict(xtepca)\n",
    "ypr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a6fc3d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "ac1=accuracy_score(ypr1,ytepca)\n",
    "model.append('NB')\n",
    "acc.append(ac1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "02770e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.94      0.96        80\n",
      "           1       0.89      0.97      0.93        40\n",
      "\n",
      "    accuracy                           0.95       120\n",
      "   macro avg       0.94      0.96      0.95       120\n",
      "weighted avg       0.95      0.95      0.95       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ypr1,ytepca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ebfd5614",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=39, random_state=2)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LR=LogisticRegression(max_iter=39,random_state=2)\n",
    "LR.fit(xtrpca,ytrpca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "57e83eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypr2=LR.predict(xtepca)\n",
    "ypr2\n",
    "ac2=accuracy_score(ypr2,ytepca)\n",
    "model.append('LR')\n",
    "acc.append(ac2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "11691c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99        74\n",
      "           1       1.00      0.96      0.98        46\n",
      "\n",
      "    accuracy                           0.98       120\n",
      "   macro avg       0.99      0.98      0.98       120\n",
      "weighted avg       0.98      0.98      0.98       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ypr2,ytepca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3fe12380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95        69\n",
      "           1       1.00      0.86      0.93        51\n",
      "\n",
      "    accuracy                           0.94       120\n",
      "   macro avg       0.95      0.93      0.94       120\n",
      "weighted avg       0.95      0.94      0.94       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "MLP=MLPClassifier(activation='relu',learning_rate='constant')\n",
    "MLP.fit(xtrpca,ytrpca)\n",
    "ypr3=MLP.predict(xtepca)\n",
    "ypr3\n",
    "ac3=accuracy_score(ypr3,ytepca)\n",
    "print(classification_report(ypr3,ytepca))\n",
    "model.append('MLP')\n",
    "acc.append(ac3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "985c1883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "ensemble_model=VotingClassifier(estimators=[('MLP',m3),\n",
    "    ('logistic_regression',m2)],voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "194326f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('MLP', MLPClassifier()),\n",
       "                             ('logistic_regression',\n",
       "                              LogisticRegression(max_iter=39, random_state=2))],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_model.fit(xtrpca,ytrpca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6545113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = ensemble_model.predict(xtepca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "26c252dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the ensemble model: 0.97\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(ytepca, predictions)\n",
    "print(f'Accuracy of the ensemble model: {accuracy:.2f}')\n",
    "model.append('ensemble_model')\n",
    "acc.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2b3169d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        72\n",
      "           1       1.00      0.92      0.96        48\n",
      "\n",
      "    accuracy                           0.97       120\n",
      "   macro avg       0.97      0.96      0.96       120\n",
      "weighted avg       0.97      0.97      0.97       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictions,ytepca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "60c33716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Accuracy Comparison'}, xlabel='Accuracy', ylabel='Algorithm'>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[10,5],dpi = 100)\n",
    "plt.title('Accuracy Comparison')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Algorithm')\n",
    "sns.barplot(x=acc,y=model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
